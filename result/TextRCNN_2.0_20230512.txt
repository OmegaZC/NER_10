C:\Users\Infantechan\.conda\envs\torch_prj_env\python.exe C:/Users/Infantechan/Desktop/Chinese-Text-Classification-Pytorch-all/run.py --model=TextRCNN
Loading data...
Vocab size: 4762
39276it [00:12, 3207.79it/s]
2060it [00:00, 3313.70it/s]
2062it [00:00, 3264.45it/s]
C:\Users\Infantechan\.conda\envs\torch_prj_env\lib\site-packages\torch\nn\modules\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=1.0 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
Time usage: 0:00:14
<bound method Module.parameters of Model(
  (embedding): Embedding(4762, 300)
  (lstm): LSTM(300, 384, batch_first=True, dropout=1.0, bidirectional=True)
  (maxpool): MaxPool1d(kernel_size=1024, stride=1024, padding=0, dilation=1, ceil_mode=False)
  (fc): Linear(in_features=1068, out_features=5, bias=True)
)>
Epoch [1/200]
C:\Users\Infantechan\.conda\envs\torch_prj_env\lib\site-packages\transformers\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
C:\Users\Infantechan\.conda\envs\torch_prj_env\lib\site-packages\torch\optim\lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
  0%|          | 1/307 [00:03<16:43,  3.28s/it]
Iter:      0,  Train Loss: 1.601,  Train Acc: 24.2188%,  Val Loss: 1.382,  Val Acc: 58.4951%,  Time: 0:00:03 *
 33%|███▎      | 101/307 [00:43<03:36,  1.05s/it]
Iter:    100,  Train Loss: 0.9058,  Train Acc: 67.1875%,  Val Loss: 0.7266,  Val Acc: 73.2524%,  Time: 0:00:44 *
 65%|██████▌   | 201/307 [01:23<01:51,  1.05s/it]
Iter:    200,  Train Loss: 0.6004,  Train Acc: 75.0000%,  Val Loss: 0.6038,  Val Acc: 74.7573%,  Time: 0:01:24 *
 98%|█████████▊| 301/307 [02:03<00:06,  1.05s/it]
Iter:    300,  Train Loss: 0.4824,  Train Acc: 83.5938%,  Val Loss: 0.4479,  Val Acc: 82.8641%,  Time: 0:02:03 *
100%|██████████| 307/307 [02:05<00:00,  2.45it/s]
  0%|          | 0/307 [00:00<?, ?it/s]Epoch [2/200]
 31%|███       | 94/307 [00:37<03:40,  1.04s/it]
Iter:    400,  Train Loss: 0.3979,  Train Acc: 89.8438%,  Val Loss: 0.4555,  Val Acc: 82.4757%,  Time: 0:02:43 
 63%|██████▎   | 194/307 [01:17<01:58,  1.05s/it]
Iter:    500,  Train Loss: 0.2321,  Train Acc: 92.9688%,  Val Loss: 0.3902,  Val Acc: 85.2913%,  Time: 0:03:23 *
 96%|█████████▌| 294/307 [01:58<00:13,  1.06s/it]
Iter:    600,  Train Loss: 0.2342,  Train Acc: 92.9688%,  Val Loss: 0.338,  Val Acc: 87.6214%,  Time: 0:04:04 *
100%|██████████| 307/307 [02:03<00:00,  2.49it/s]
Epoch [3/200]
 28%|██▊       | 87/307 [00:35<03:51,  1.05s/it]
Iter:    700,  Train Loss: 0.133,  Train Acc: 96.0938%,  Val Loss: 0.3346,  Val Acc: 88.0097%,  Time: 0:04:44 *
 61%|██████    | 187/307 [01:14<02:03,  1.03s/it]
Iter:    800,  Train Loss: 0.1172,  Train Acc: 96.0938%,  Val Loss: 0.3643,  Val Acc: 87.4272%,  Time: 0:05:23 
 93%|█████████▎| 287/307 [01:54<00:20,  1.03s/it]
Iter:    900,  Train Loss: 0.1193,  Train Acc: 98.4375%,  Val Loss: 0.3177,  Val Acc: 89.3204%,  Time: 0:06:03 *
100%|██████████| 307/307 [02:02<00:00,  2.52it/s]
Epoch [4/200]
 26%|██▌       | 80/307 [00:32<03:57,  1.05s/it]
Iter:   1000,  Train Loss: 0.09871,  Train Acc: 97.6562%,  Val Loss: 0.3158,  Val Acc: 89.5631%,  Time: 0:06:43 *
 59%|█████▊    | 180/307 [01:11<02:10,  1.02s/it]
Iter:   1100,  Train Loss: 0.1473,  Train Acc: 96.0938%,  Val Loss: 0.3745,  Val Acc: 88.1553%,  Time: 0:07:23 
 91%|█████████ | 280/307 [01:51<00:27,  1.03s/it]
Iter:   1200,  Train Loss: 0.1229,  Train Acc: 97.6562%,  Val Loss: 0.3172,  Val Acc: 89.3204%,  Time: 0:08:02 
100%|██████████| 307/307 [02:01<00:00,  2.53it/s]
Epoch [5/200]
 24%|██▍       | 73/307 [00:29<04:03,  1.04s/it]
Iter:   1300,  Train Loss: 0.1319,  Train Acc: 96.8750%,  Val Loss: 0.3695,  Val Acc: 89.2233%,  Time: 0:08:42 
 56%|█████▋    | 173/307 [01:10<02:20,  1.05s/it]
Iter:   1400,  Train Loss: 0.0704,  Train Acc: 97.6562%,  Val Loss: 0.3446,  Val Acc: 89.5631%,  Time: 0:09:22 
 89%|████████▉ | 273/307 [01:49<00:36,  1.06s/it]
Iter:   1500,  Train Loss: 0.1177,  Train Acc: 96.8750%,  Val Loss: 0.3142,  Val Acc: 90.0485%,  Time: 0:10:02 *
100%|██████████| 307/307 [02:02<00:00,  2.50it/s]
  0%|          | 0/307 [00:00<?, ?it/s]Epoch [6/200]
 21%|██▏       | 66/307 [00:26<04:08,  1.03s/it]
Iter:   1600,  Train Loss: 0.05511,  Train Acc: 98.4375%,  Val Loss: 0.3482,  Val Acc: 88.8835%,  Time: 0:10:42 
 54%|█████▍    | 166/307 [01:06<02:24,  1.02s/it]
Iter:   1700,  Train Loss: 0.07075,  Train Acc: 97.6562%,  Val Loss: 0.3196,  Val Acc: 90.3883%,  Time: 0:11:21 
 87%|████████▋ | 266/307 [01:45<00:42,  1.04s/it]
Iter:   1800,  Train Loss: 0.07307,  Train Acc: 99.2188%,  Val Loss: 0.3317,  Val Acc: 89.9029%,  Time: 0:12:01 
100%|██████████| 307/307 [02:00<00:00,  2.54it/s]
Epoch [7/200]
 19%|█▉        | 59/307 [00:24<04:18,  1.04s/it]
Iter:   1900,  Train Loss: 0.06646,  Train Acc: 98.4375%,  Val Loss: 0.3535,  Val Acc: 89.4175%,  Time: 0:12:40 
 52%|█████▏    | 159/307 [01:03<02:36,  1.05s/it]
Iter:   2000,  Train Loss: 0.1261,  Train Acc: 97.6562%,  Val Loss: 0.3556,  Val Acc: 88.9320%,  Time: 0:13:19 
 84%|████████▍ | 259/307 [01:43<00:49,  1.04s/it]
Iter:   2100,  Train Loss: 0.1333,  Train Acc: 96.0938%,  Val Loss: 0.3404,  Val Acc: 89.6117%,  Time: 0:13:59 
100%|██████████| 307/307 [02:00<00:00,  2.54it/s]
  0%|          | 0/307 [00:00<?, ?it/s]Epoch [8/200]
 17%|█▋        | 52/307 [00:21<04:24,  1.04s/it]
Iter:   2200,  Train Loss: 0.0686,  Train Acc: 98.4375%,  Val Loss: 0.3706,  Val Acc: 89.0291%,  Time: 0:14:38 
 50%|████▉     | 152/307 [01:00<02:38,  1.02s/it]
Iter:   2300,  Train Loss: 0.02505,  Train Acc: 100.0000%,  Val Loss: 0.3417,  Val Acc: 89.6602%,  Time: 0:15:18 
 82%|████████▏ | 252/307 [01:40<00:56,  1.03s/it]
Iter:   2400,  Train Loss: 0.03149,  Train Acc: 98.4375%,  Val Loss: 0.3505,  Val Acc: 89.7573%,  Time: 0:15:57 
100%|██████████| 307/307 [02:00<00:00,  2.54it/s]
Epoch [9/200]
 14%|█▍        | 44/307 [00:19<01:53,  2.31it/s]

Iter:   2500,  Train Loss: 0.02052,  Train Acc: 99.2188%,  Val Loss: 0.3587,  Val Acc: 89.4660%,  Time: 0:16:37 
No optimization for a long time, auto-stopping...
Test Loss:   0.3,  Test Acc: 90.59%
Precision, Recall and F1-Score...
              precision    recall  f1-score   support

        一般不满     0.7048    0.7708    0.7363        96
        比较不满     0.8430    0.8395    0.8412       486
   非常不满-渠道敏感     0.9860    0.9942    0.9901      1206
   非常不满-费用敏感     0.7647    0.7222    0.7429       108
   非常不满-服务敏感     0.7032    0.6566    0.6791       166

    accuracy                         0.9059      2062
   macro avg     0.8003    0.7967    0.7979      2062
weighted avg     0.9049    0.9059    0.9052      2062

Confusion Matrix...
[[  74   17    0    1    4]
 [  22  408    9   14   33]
 [   0    3 1199    0    4]
 [   1   19    5   78    5]
 [   8   37    3    9  109]]
Time usage: 0:00:02
[W CUDAGuardImpl.h:46] Warning: CUDA warning: driver shutting down (function uncheckedGetDevice)
[W CUDAGuardImpl.h:62] Warning: CUDA warning: driver shutting down (function uncheckedSetDevice)

Process finished with exit code 0