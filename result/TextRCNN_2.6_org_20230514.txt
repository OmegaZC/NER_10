C:\Users\Infantechan\.conda\envs\torch_prj_env\python.exe C:\Users\Infantechan\Desktop\Chinese-Text-Classification-Pytorch-all\run.py --model=TextRCNN 
Loading data...
Vocab size: 4762
16503it [00:05, 3262.68it/s]
2060it [00:00, 3324.41it/s]
2062it [00:00, 3135.49it/s]
C:\Users\Infantechan\.conda\envs\torch_prj_env\lib\site-packages\torch\nn\modules\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1
  warnings.warn("dropout option adds dropout after all but last "
Time usage: 0:00:06
C:\Users\Infantechan\.conda\envs\torch_prj_env\lib\site-packages\transformers\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|          | 0/129 [00:00<?, ?it/s]<bound method Module.parameters of Model(
  (embedding): Embedding(4762, 300)
  (lstm): LSTM(300, 384, batch_first=True, dropout=0.6, bidirectional=True)
  (maxpool): MaxPool1d(kernel_size=1024, stride=1024, padding=0, dilation=1, ceil_mode=False)
  (fc): Linear(in_features=1068, out_features=5, bias=True)
)>
Epoch [1/200]

Iter:      0,  Train Loss: 1.634,  Train Acc: 8.5938%,  Val Loss: 1.326,  Val Acc: 58.4951%,  Time: 0:00:04 *
 78%|███████▊  | 101/129 [00:42<00:28,  1.03s/it]
Iter:    100,  Train Loss: 0.6734,  Train Acc: 74.2188%,  Val Loss: 0.6178,  Val Acc: 76.3107%,  Time: 0:00:43 *
100%|██████████| 129/129 [00:52<00:00,  2.43it/s]
Epoch [2/200]
 56%|█████▌    | 72/129 [00:28<00:59,  1.04s/it]
Iter:    200,  Train Loss: 0.3495,  Train Acc: 89.0625%,  Val Loss: 0.4314,  Val Acc: 84.6602%,  Time: 0:01:22 *
100%|██████████| 129/129 [00:49<00:00,  2.60it/s]
  0%|          | 0/129 [00:00<?, ?it/s]Epoch [3/200]
 33%|███▎      | 43/129 [00:18<01:29,  1.04s/it]
Iter:    300,  Train Loss: 0.2772,  Train Acc: 89.8438%,  Val Loss: 0.3235,  Val Acc: 87.4757%,  Time: 0:02:01 *
100%|██████████| 129/129 [00:49<00:00,  2.60it/s]
Epoch [4/200]
 11%|█         | 14/129 [00:07<02:01,  1.06s/it]
Iter:    400,  Train Loss: 0.2011,  Train Acc: 90.6250%,  Val Loss: 0.313,  Val Acc: 88.3010%,  Time: 0:02:40 *
 88%|████████▊ | 114/129 [00:48<00:15,  1.04s/it]
Iter:    500,  Train Loss: 0.2998,  Train Acc: 89.0625%,  Val Loss: 0.3027,  Val Acc: 88.7864%,  Time: 0:03:21 *
100%|██████████| 129/129 [00:54<00:00,  2.37it/s]
  0%|          | 0/129 [00:00<?, ?it/s]Epoch [5/200]
 66%|██████▌   | 85/129 [00:33<00:46,  1.05s/it]
Iter:    600,  Train Loss: 0.3346,  Train Acc: 86.7188%,  Val Loss: 0.2981,  Val Acc: 88.7864%,  Time: 0:04:00 *
100%|██████████| 129/129 [00:49<00:00,  2.60it/s]
  0%|          | 0/129 [00:00<?, ?it/s]Epoch [6/200]
 43%|████▎     | 56/129 [00:22<01:17,  1.06s/it]
Iter:    700,  Train Loss: 0.1969,  Train Acc: 93.7500%,  Val Loss: 0.2778,  Val Acc: 89.7573%,  Time: 0:04:39 *
100%|██████████| 129/129 [00:49<00:00,  2.60it/s]
  0%|          | 0/129 [00:00<?, ?it/s]Epoch [7/200]
 21%|██        | 27/129 [00:12<01:44,  1.03s/it]
Iter:    800,  Train Loss: 0.278,  Train Acc: 89.8438%,  Val Loss: 0.2575,  Val Acc: 90.6796%,  Time: 0:05:18 *
 98%|█████████▊| 127/129 [00:50<00:02,  1.03s/it]
Iter:    900,  Train Loss: 0.2991,  Train Acc: 88.2812%,  Val Loss: 0.2495,  Val Acc: 91.3107%,  Time: 0:05:57 *
100%|██████████| 129/129 [00:51<00:00,  2.50it/s]
  0%|          | 0/129 [00:00<?, ?it/s]Epoch [8/200]
 76%|███████▌  | 98/129 [00:38<00:31,  1.02s/it]
Iter:   1000,  Train Loss: 0.2591,  Train Acc: 90.6250%,  Val Loss: 0.2736,  Val Acc: 89.6117%,  Time: 0:06:36 
100%|██████████| 129/129 [00:49<00:00,  2.60it/s]
  0%|          | 0/129 [00:00<?, ?it/s]Epoch [9/200]
 53%|█████▎    | 69/129 [00:27<01:02,  1.04s/it]
Iter:   1100,  Train Loss: 0.222,  Train Acc: 91.4062%,  Val Loss: 0.254,  Val Acc: 90.9709%,  Time: 0:07:15 
100%|██████████| 129/129 [00:49<00:00,  2.61it/s]
  0%|          | 0/129 [00:00<?, ?it/s]Epoch [10/200]
 31%|███       | 40/129 [00:16<01:32,  1.04s/it]
Iter:   1200,  Train Loss: 0.2069,  Train Acc: 90.6250%,  Val Loss: 0.2615,  Val Acc: 90.8738%,  Time: 0:07:54 
100%|██████████| 129/129 [00:49<00:00,  2.61it/s]
Epoch [11/200]
  9%|▊         | 11/129 [00:06<02:02,  1.04s/it]
Iter:   1300,  Train Loss: 0.184,  Train Acc: 95.3125%,  Val Loss: 0.2383,  Val Acc: 91.7476%,  Time: 0:08:32 *
 86%|████████▌ | 111/129 [00:44<00:18,  1.02s/it]
Iter:   1400,  Train Loss: 0.2193,  Train Acc: 93.7500%,  Val Loss: 0.2688,  Val Acc: 90.9223%,  Time: 0:09:11 
100%|██████████| 129/129 [00:51<00:00,  2.51it/s]
Epoch [12/200]
 64%|██████▎   | 82/129 [00:31<00:47,  1.02s/it]
Iter:   1500,  Train Loss: 0.1214,  Train Acc: 97.6562%,  Val Loss: 0.2594,  Val Acc: 91.2136%,  Time: 0:09:50 
100%|██████████| 129/129 [00:49<00:00,  2.63it/s]
Epoch [13/200]
 41%|████      | 53/129 [00:22<01:31,  1.20s/it]
Iter:   1600,  Train Loss: 0.1632,  Train Acc: 95.3125%,  Val Loss: 0.2686,  Val Acc: 90.4369%,  Time: 0:10:29 
100%|██████████| 129/129 [00:49<00:00,  2.59it/s]
  0%|          | 0/129 [00:00<?, ?it/s]Epoch [14/200]
 19%|█▊        | 24/129 [00:11<01:50,  1.05s/it]
Iter:   1700,  Train Loss: 0.1712,  Train Acc: 93.7500%,  Val Loss: 0.2571,  Val Acc: 91.1650%,  Time: 0:11:08 
 96%|█████████▌| 124/129 [00:49<00:05,  1.02s/it]
Iter:   1800,  Train Loss: 0.1262,  Train Acc: 96.0938%,  Val Loss: 0.2731,  Val Acc: 91.4078%,  Time: 0:11:46 
100%|██████████| 129/129 [00:51<00:00,  2.51it/s]
Epoch [15/200]
 74%|███████▎  | 95/129 [00:36<00:34,  1.02s/it]
Iter:   1900,  Train Loss: 0.1464,  Train Acc: 95.3125%,  Val Loss: 0.2613,  Val Acc: 91.7961%,  Time: 0:12:25 
100%|██████████| 129/129 [00:49<00:00,  2.62it/s]
Epoch [16/200]
 51%|█████     | 66/129 [00:26<01:04,  1.02s/it]
Iter:   2000,  Train Loss: 0.1313,  Train Acc: 96.0938%,  Val Loss: 0.2686,  Val Acc: 91.0194%,  Time: 0:13:03 
100%|██████████| 129/129 [00:49<00:00,  2.62it/s]
  0%|          | 0/129 [00:00<?, ?it/s]Epoch [17/200]
 29%|██▊       | 37/129 [00:15<01:34,  1.02s/it]
Iter:   2100,  Train Loss: 0.09752,  Train Acc: 98.4375%,  Val Loss: 0.2777,  Val Acc: 90.5340%,  Time: 0:13:42 
100%|██████████| 129/129 [00:49<00:00,  2.61it/s]
  0%|          | 0/129 [00:00<?, ?it/s]Epoch [18/200]
  6%|▌         | 8/129 [00:05<02:08,  1.06s/it]
Iter:   2200,  Train Loss: 0.08261,  Train Acc: 96.8750%,  Val Loss: 0.2848,  Val Acc: 91.2136%,  Time: 0:14:21 
 83%|████████▎ | 107/129 [00:43<00:08,  2.45it/s]

Iter:   2300,  Train Loss: 0.06531,  Train Acc: 97.6562%,  Val Loss: 0.271,  Val Acc: 91.4563%,  Time: 0:14:59 
No optimization for a long time, auto-stopping...
Test Loss:  0.25,  Test Acc: 91.13%
Precision, Recall and F1-Score...
              precision    recall  f1-score   support

        一般不满     0.7000    0.7292    0.7143        96
        比较不满     0.8317    0.8951    0.8622       486
   非常不满-渠道敏感     0.9876    0.9909    0.9892      1206
   非常不满-费用敏感     0.8283    0.7593    0.7923       108
   非常不满-服务敏感     0.7462    0.5843    0.6554       166

    accuracy                         0.9113      2062
   macro avg     0.8188    0.7917    0.8027      2062
weighted avg     0.9097    0.9113    0.9093      2062

Confusion Matrix...
[[  70   21    0    1    4]
 [  18  435    6    7   20]
 [   1    3 1195    2    5]
 [   2   17    3   82    4]
 [   9   47    6    7   97]]
Time usage: 0:00:02
[W CUDAGuardImpl.h:46] Warning: CUDA warning: driver shutting down (function uncheckedGetDevice)
[W CUDAGuardImpl.h:62] Warning: CUDA warning: driver shutting down (function uncheckedSetDevice)

Process finished with exit code 0

